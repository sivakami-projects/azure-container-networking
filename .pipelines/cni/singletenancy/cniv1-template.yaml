parameters:
  dependsOn: ""
  name: ""
  clusterType: ""
  clusterName: ""
  nodeCount: ""
  nodeCountWin: ""
  vmSize: ""
  vmSizeWin: ""
  os: ""
  arch: ""
  scaleup: 100
  iterations: 3
  os_version: ""
  osSKU: Ubuntu
  osSkuWin: Windows2022

# CNIv1
# + Should be able to scale up/down the pods successfully certain number of times.
# + Node reboot scenarios should be covered.
# + The azure-vnet state should be validated with that of CNI state.
# + Pods should have ips assigned and connectivity/datapath test should be present.
# Windows
# + The HNS state should be validated with that of CNI state.
# + All CNI E2E is re-ran after HNS service is restarted

# If ensures that only linux template calls are compared against the below condition
# Condition confirms that:
# Previous job has reported Succeeded. Previous job is currently setup which controls variable assignment and we are dependent on its success.
# CONTROL_OS either contains 'linux' or 'all' and CONTROL_CNI either contains 'cniv1' or 'all'. Both must be true and are not case sensitive
stages:
  - ${{ if eq(parameters.os, 'linux') }}:
    - stage: create_${{ parameters.name }}
      condition: and( succeeded(), and( or( contains(variables.CONTROL_CNI, 'cniv1'), contains(variables.CONTROL_CNI, 'all') ), or( contains(variables.CONTROL_OS, 'linux'), contains(variables.CONTROL_OS, 'all') ) ) )
      variables:
        commitID: $[ stagedependencies.setup.env.outputs['SetEnvVars.commitID'] ]
        npmVersion: $[ stagedependencies.setup.env.outputs['SetEnvVars.npmVersion'] ]
      pool:
        name: $(BUILD_POOL_NAME_DEFAULT)
      dependsOn:
        - setup
      displayName: "Create Cluster - ${{ parameters.clusterName }}"
      jobs:
        - job: create_aks_cluster_with_${{ parameters.name }}
          steps:
            - template: ../load-test-templates/create-cluster-template.yaml
              parameters:
                clusterType: ${{ parameters.clusterType }}
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                nodeCount: ${{ parameters.nodeCount }}
                vmSize: ${{ parameters.vmSize }}
                vmSizeWin: ${{ parameters.vmSizeWin }}
                region: $(LOCATION)
                osSKU: ${{ parameters.osSKU }}

# If ensures that only windows template calls are compared against the below condition
# Condition confirms that:
# Previous job has reported Succeeded. Previous job is currently setup which controls variable assignment and we are dependent on its success.
# CONTROL_OS either contains 'windows' or 'all' and CONTROL_CNI either contains 'cniv1' or 'all'. Both must be true and are not case sensitive
  - ${{ if eq(parameters.os, 'windows') }}:
    - stage: create_${{ parameters.name }}
      condition: and( succeeded(), and( or( contains(variables.CONTROL_CNI, 'cniv1'), contains(variables.CONTROL_CNI, 'all') ), or( contains(variables.CONTROL_OS, 'windows'), contains(variables.CONTROL_OS, 'all') ) ) )
      variables:
        commitID: $[ stagedependencies.setup.env.outputs['SetEnvVars.commitID'] ]
        npmVersion: $[ stagedependencies.setup.env.outputs['SetEnvVars.npmVersion'] ]
      pool:
        name: $(BUILD_POOL_NAME_DEFAULT)
      dependsOn:
        - setup
      displayName: "Create Cluster - ${{ parameters.clusterName }}"
      jobs:
        - job: create_aks_cluster_with_${{ parameters.name }}
          steps:
            - template: ../load-test-templates/create-cluster-template.yaml
              parameters:
                clusterType: ${{ parameters.clusterType }}
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                nodeCount: ${{ parameters.nodeCount }}
                nodeCountWin: ${{ parameters.nodeCountWin }}
                vmSize: ${{ parameters.vmSize }}
                vmSizeWin: ${{ parameters.vmSizeWin }}
                region: $(LOCATION)
                osSKU: ${{ parameters.osSKU }}
                os: ${{ parameters.os }}
                osSkuWin: ${{ parameters.osSkuWin }}

# Conditions for below E2E test scenarios confirm that:
# Pipeline has not been canceled and that the previous job has reports anything other than failure(Succeeded, SuccededWithIssues, Skipped). Previous job is declared by dependsOn:
# CONTROL_SCENARIO either contains 'all' or its respective scenario 'npm', 'scaleTest', 'restartNode', 'restartCNS'. It is not case sensitive
# Ex. CONTROL_SCENARIO = "restartnode SCALETEST" will only run Scale Test and Restart Test.
  - stage: ${{ parameters.name }}
    variables:
      commitID: $[ stagedependencies.setup.env.outputs['SetEnvVars.commitID'] ]
      npmVersion: $[ stagedependencies.setup.env.outputs['SetEnvVars.npmVersion'] ]
    pool:
      name: $(BUILD_POOL_NAME_DEFAULT)
    dependsOn:
      - create_${{ parameters.name }}
      - publish
      - setup
    displayName: "CNIv1 Test - ${{ parameters.clusterName }}"
    jobs:
      - job: update_cni
        steps:
          - task: AzureCLI@1
            inputs:
              azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
              scriptLocation: "inlineScript"
              scriptType: "bash"
              addSpnToEnvironment: true
              inlineScript: |
                set -ex
                make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}-$(commitID)
                echo "Upload CNI"
                echo "Deploying on Linux nodes"
                if [ "${{parameters.os}}" == "windows" ]; then
                  export CNI_IMAGE=$(make cni-image-name-and-tag OS='linux' ARCH=${{ parameters.arch }} CNI_VERSION=$(make cni-version))
                  echo "CNI image: $CNI_IMAGE"
                  envsubst '${CNI_IMAGE}' < ./test/integration/manifests/cni/cni-installer-v1.yaml | kubectl apply -f -
                  kubectl rollout status daemonset/azure-cni -n kube-system
                  echo "Deploying on windows nodes"
                  export CNI_IMAGE=$(make cni-image-name-and-tag OS='windows' ARCH=${{ parameters.arch }}  OS_VERSION=${{ parameters.os_version }} CNI_VERSION=$(make cni-version))
                  echo "CNI image: $CNI_IMAGE"
                  envsubst '${CNI_IMAGE}' < ./test/integration/manifests/cni/cni-installer-v1-windows.yaml | kubectl apply -f -
                  kubectl rollout status daemonset/azure-cni-windows -n kube-system
                else
                  export CNI_IMAGE=$(make cni-image-name-and-tag OS=${{ parameters.os }} ARCH=${{ parameters.arch }} CNI_VERSION=$(make cni-version))
                  echo "CNI image: $CNI_IMAGE"
                  envsubst '${CNI_IMAGE}' < ./test/integration/manifests/cni/cni-installer-v1.yaml | kubectl apply -f -
                  kubectl rollout status daemonset/azure-cni -n kube-system
                fi
                kubectl get pods -A -owide
            name: "deployCNI"
            displayName: "Deploy CNI"
      - template: ../../npm/npm-cni-integration-test.yaml
        parameters:
          clusterName: ${{ parameters.clusterName }}-$(commitID)
          os: ${{ parameters.os }}
          os_version: ${{ parameters.os_version }}
          sub: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
          tag: $(npmVersion)
          dependsOn: update_cni
      - job: deploy_pods
        condition: and( and( not(canceled()), not(failed()) ), or( contains(variables.CONTROL_SCENARIO, 'scaleTest') , contains(variables.CONTROL_SCENARIO, 'all') ) )
        displayName: "Scale Test"
        dependsOn: npm_k8se2e
        steps:
          - template: ../load-test-templates/pod-deployment-template.yaml
            parameters:
              clusterName: ${{ parameters.clusterName }}-$(commitID)
              scaleup: ${{ parameters.scaleup }}
              os: ${{ parameters.os }}
              iterations: ${{ parameters.iterations }}
              nodeCount: ${{ parameters.nodeCount }}
              cni: cniv1
          - template: ../load-test-templates/validate-state-template.yaml
            parameters:
              clusterName: ${{ parameters.clusterName }}-$(commitID)
              os: ${{ parameters.os }}
              cni: cniv1
      - job: restart_nodes
        condition: and( and( not(canceled()), not(failed()) ), or( contains(variables.CONTROL_SCENARIO, 'restartNode') , contains(variables.CONTROL_SCENARIO, 'all') ) )
        displayName: "Restart Test"
        dependsOn: deploy_pods
        steps:
          - template: ../load-test-templates/restart-node-template.yaml
            parameters:
              clusterName: ${{ parameters.clusterName }}-$(commitID)
              os: ${{ parameters.os }}
              cni: cniv1
          - template: ../load-test-templates/validate-state-template.yaml
            parameters:
              clusterName: ${{ parameters.clusterName }}-$(commitID)
              os: ${{ parameters.os }}
              cni: cniv1
              restartCase: "true"
      - job: recover
        displayName: "Recover Resources"
        dependsOn: restart_nodes
        steps:
          - task: AzureCLI@1
            inputs:
              azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
              scriptLocation: "inlineScript"
              scriptType: "bash"
              addSpnToEnvironment: true
              inlineScript: |
                echo "Delete load-test Namespace"
                make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}-$(commitID)
                kubectl get ns --no-headers | grep -v 'kube\|default' | awk '{print $1}'
                delete=`kubectl get ns --no-headers | grep -v 'kube\|default' | awk '{print $1}'`
                kubectl delete ns $delete
                kubectl cluster-info
                kubectl get po -owide -A
            name: "recover"
            displayName: "Delete test Namespaces"
      - template: ../k8s-e2e/k8s-e2e-job-template.yaml
        parameters:
          sub: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
          clusterName: ${{ parameters.clusterName }}-$(commitID)
          os: ${{ parameters.os }}
          dependsOn: recover
          datapath: true
          dns: true
          portforward: true
          hybridWin: true
          service: true
          hostport: true
      - job: failedE2ELogs
        displayName: "Failure Logs"
        dependsOn:
          - update_cni
          - npm_k8se2e
          - deploy_pods
          - restart_nodes
          - recover
          - cni_${{ parameters.os }}
        condition: failed()
        steps:
          - template: ../../templates/log-template.yaml
            parameters:
              clusterName: ${{ parameters.clusterName }}-$(commitID)
              os: ${{ parameters.os }}

# Conditions for below E2E test scenarios confirm that:
# Pipeline has not been canceled and that the previous job has reports anything other than failure(Succeeded, SuccededWithIssues, Skipped). Previous job is declared by dependsOn:
# CONTROL_SCENARIO either contains 'all' or its respective scenario 'npm', 'scaleTest', 'restartNode', 'restartCNS'. It is not case sensitive
# Ex. CONTROL_SCENARIO = "restartnode SCALETEST" will only run Scale Test and Restart Test.
  - ${{ if eq(parameters.os, 'windows') }}:
    - stage: ${{ parameters.name }}_HNS
      variables:
        commitID: $[ stagedependencies.setup.env.outputs['SetEnvVars.commitID'] ]
        npmVersion: $[ stagedependencies.setup.env.outputs['SetEnvVars.npmVersion'] ]
      pool:
        name: $(BUILD_POOL_NAME_DEFAULT)
      dependsOn:
        - ${{ parameters.name }}
        - setup
      displayName: "HNS Test - ${{ parameters.clusterName }}"
      jobs:
        - job: restart_hns
          displayName: "Restart and Validate HNS"
          condition: and( and( not(canceled()), not(failed()) ), ${{ eq(parameters.os, 'windows') }} )
          steps:
            - template: ../load-test-templates/restart-hns-template.yaml
              parameters:
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                cni: cniv1
        - job: deploy_podsHNS
          condition: and( and( not(canceled()), not(failed()) ), or( contains(variables.CONTROL_SCENARIO, 'scaleTest') , contains(variables.CONTROL_SCENARIO, 'all') ) )
          displayName: "Scale Test"
          dependsOn: restart_hns
          steps:
            - template: ../load-test-templates/pod-deployment-template.yaml
              parameters:
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                scaleup: ${{ parameters.scaleup }}
                os: ${{ parameters.os }}
                iterations: ${{ parameters.iterations }}
                nodeCount: ${{ parameters.nodeCount }}
                jobName: deploy_podsHNS
            - template: ../load-test-templates/validate-state-template.yaml
              parameters:
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                os: ${{ parameters.os }}
                cni: cniv1
        - job: restart_nodesHNS
          condition: and( and( not(canceled()), not(failed()) ), or( contains(variables.CONTROL_SCENARIO, 'restartNode') , contains(variables.CONTROL_SCENARIO, 'all') ) )
          displayName: "Restart Test"
          dependsOn: deploy_podsHNS
          steps:
            - template: ../load-test-templates/restart-node-template.yaml
              parameters:
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                os: ${{ parameters.os }}
                cni: cniv1
                jobName: restart_nodesHNS
            - template: ../load-test-templates/validate-state-template.yaml
              parameters:
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                os: ${{ parameters.os }}
                cni: cniv1
                restartCase: "true"
        - job: recover
          displayName: "Recover Resources"
          dependsOn: restart_nodesHNS
          steps:
            - task: AzureCLI@1
              inputs:
                azureSubscription: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
                scriptLocation: "inlineScript"
                scriptType: "bash"
                addSpnToEnvironment: true
                inlineScript: |
                  echo "Delete load-test Namespace"
                  make -C ./hack/aks set-kubeconf AZCLI=az CLUSTER=${{ parameters.clusterName }}-$(commitID)
                  kubectl get ns --no-headers | grep -v 'kube\|default' | awk '{print $1}'
                  delete=`kubectl get ns --no-headers | grep -v 'kube\|default' | awk '{print $1}'`
                  kubectl delete ns $delete
                  kubectl cluster-info
                  kubectl get po -owide -A
              name: "recover"
              displayName: "Delete test Namespaces"
        - template: ../k8s-e2e/k8s-e2e-job-template.yaml
          parameters:
            sub: $(BUILD_VALIDATIONS_SERVICE_CONNECTION)
            clusterName: ${{ parameters.clusterName }}-$(commitID)
            os: ${{ parameters.os }}
            dependsOn: recover
            datapath: true
            dns: true
            portforward: true
            hybridWin: true
            service: true
            hostport: true
        - job: failedE2ELogsHNS
          displayName: "Failure Logs"
          dependsOn:
            - restart_hns
            - restart_nodesHNS
            - deploy_podsHNS
            - recover
            - cni_${{ parameters.os }}
          condition: failed()
          steps:
            - template: ../../templates/log-template.yaml
              parameters:
                clusterName: ${{ parameters.clusterName }}-$(commitID)
                os: ${{ parameters.os }}
                jobName: failedE2ELogsHNS
